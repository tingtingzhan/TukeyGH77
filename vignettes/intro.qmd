---
title: "Tukey"
author: Tingting Zhan
format: 
  html:
    page-layout: full
    html-math-method: katex
toc: true
toc-location: left
toc-depth: 4
toc-title: ''
editor: visual
vignette: >
  %\VignetteIndexEntry{intro}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
---

# Introduction

This vignette of package **`TukeyGH77`** ([CRAN](https://CRAN.R-project.org/package=TukeyGH77), [Github](https://github.com/tingtingzhan/TukeyGH77)) documents ..

## Note to Users

Examples in this vignette require that the `search` path has

```{r}
library(TukeyGH77)
```

```{r}
#| echo: false
library(knitr) # for tables in this vignette
op = par(no.readonly = TRUE)
#options(mc.cores = 1L) # for CRAN submission
```

## Terms and Abbreviations

```{r}
#| echo: false
#| results: asis
c(
  '[|>](https://search.r-project.org/R/refmans/base/html/pipeOp.html)', 'Forward pipe operator introduced in `R` 4.1.0', 
  '[`all.equal.numeric`](https://search.r-project.org/R/refmans/base/html/all.equal.html)', 'Test of near equality',
  '`CRAN`, `R`', '[The Comprehensive R Archive Network](https://cran.r-project.org)',
  '[`curve`](https://search.r-project.org/R/refmans/graphics/html/curve.html)', 'Function plots',
  '[`.lm.fit`](https://search.r-project.org/R/refmans/stats/html/lmfit.html)', 'Least squares regression, the internal workhorse',
  '[`mad`](https://search.r-project.org/R/refmans/stats/html/mad.html)', 'Median Absolute Deviation',
  '[`median`](https://search.r-project.org/R/refmans/stats/html/median.html)', 'Median',
  '[`dnorm`, `pnorm`, `qnorm`, `rnorm`](https://search.r-project.org/R/refmans/stats/html/Normal.html)', 'Normal Distribution',
  '[`optim`](https://search.r-project.org/R/refmans/stats/html/optim.html), [`optimize`](https://search.r-project.org/R/refmans/stats/html/optimize.html)', 'Optimization',
  '[`search`](https://search.r-project.org/R/refmans/base/html/search.html)', 'Search path',
  '[`seed`](https://search.r-project.org/R/refmans/base/html/Random.html)', 'Random number generation seed'
) |>
  matrix(nrow = 2L, dimnames = list(c('Term / Abbreviation', 'Description'), NULL)) |>
  t.default() |>
  as.data.frame.matrix() |> 
  kable() 
```

# Tukey's $g$-&-$h$ Transformation

## Tukey's Transformation

Tukey's $g$-&-$h$ random variable $t_{gh}$ \citep{Tukey77} is a monotone transformation of the standard normal variable $z$ \cite{Hoaglin85}, $$
t_{gh} =
\begin{cases}
z\cdot G_g(z) & g\neq 0,\ g\text{-distribution} \\
z\cdot H_h(z) & h>0,\ h\text{-distribution} \\
z\cdot G_g(z)\cdot H_h(z) & g\neq 0,h>0,\ gh\text{-distribution} \\
\end{cases}
$$ with skewness

$$
G_g(z)=
\begin{cases}
(e^{gz}-1)/gz & g\neq 0\\\
\displaystyle\lim_{g\rightarrow 0} G_{g \ne 0}(z)=1 & g=0
\end{cases}
$$ and kurtosis

$$
H_h(z)=
\begin{cases}
e^{hz^2/2} & h>0\\
1 & h =0
\end{cases}
$$ Tukey's transformation function `z2gh()` transforms the standard normal variable $z$ to Tukey's $g$-&-$h$ random variable $t_{gh}$.

```{r}
set.seed(15); rnorm(1e3L) |>
  z2gh(g = .5, h = .1) |>
  hist(breaks = 30L, xlab = NULL, main = c('Tukey(g=.5, h=.1)'))
```

## Inverse Tukey's Transformation

Inverse Tukey's transformation $\zeta_{gh}$

$$
\zeta_{gh}(t) =
\begin{cases}
g^{-1}\ln(gt+1), & g\neq0 \\
z:\ z\cdot H_h(z) = t, & h>0 \\
z:\ z\cdot G_g(z)\cdot H_h(z) = t, & g\neq 0,\ h>0 
\end{cases}
$$ does not have a closed analytical form when $h>0$, but the numerical solution could be found by using a root-finding algorithm \citep{Brent71}.

Inverse Tukey's transformation function `gh2z()` transforms the Tukey's $g$-&-$h$ random variable $t_{gh}$ to standard normal variable $z$. A closed analytical form of $\zeta_{gh}$ exists when $h=0$, thus the discrepancy is merely a floating-point error.

```{r}
set.seed(24); x = rnorm(1e3L)
x |> 
  z2gh(g = .3, h = 0) |> 
  gh2z(g = .3, h = 0) |> 
  all.equal.numeric(target = x, tolerance = .Machine$double.eps)
```

We have to rely on a root-finding algorithm when $h>0$. To find a numerical solution for standard normal quantile $z$, we need only to search the interval of $(-8.3, 8.3)$, as

```{r}
stopifnot(identical(pnorm(8.3), 1))
```

Nonetheless, the root-finding algorithm would have much larger discrepancy (which can be controlled by parameter `tol` inside function `gh2z()`).

```{r}
set.seed(91); x = rnorm(1e3L)
x |> 
  z2gh(g = 0, h = .1) |> 
  gh2z(g = 0, h = .1) |> 
  all.equal.numeric(target = x, tolerance = .Machine$double.eps)
```

```{r}
set.seed(56); x = rnorm(1e3L)
x |> 
  z2gh(g = .3, h = .1) |> 
  gh2z(g = .3, h = .1) |> 
  all.equal.numeric(target = x, tolerance = .Machine$double.eps)
```

# Tukey $g$-&-$h$ Distribution

## Probability

The distribution function is $F_{gh}(t)=\text{Pr}(T_{gh} \le t)=\text{Pr}\left(Z \le \zeta_{gh}(t)\right)$.

```{r}
curve(pGH(x, g = .3, h = .1), from = -2.5, to = 3.5, n = 501L, ylab = 'Probability')
```

## Density

Density function $f_{gh}$ has a closed analytical form in terms of $\zeta_{gh}$,

$$
f_{gh}(t) = \dfrac{e^{-z^2/2}}{\sqrt{2\pi}\cdot\partial t_{gh}/\partial z}\Bigg|_{z=\zeta_{gh}(t)}
$$

where

$$
\dfrac{\partial t_{gh}}{\partial z}=
\begin{cases}
e^{gz}, & g\neq0\\
e^{hz^2/2}(1+hz^2), & h>0 \\
e^{hz^2/2}\left(e^{gz}+g^{-1}hz(e^{gz}-1)\right), & g\neq 0,\ h>0 
\end{cases}
$$

Note that when $h=0$, domain has a bound.

```{r}
curve(dGH(x, g = 1, h = 0), from = -1.2, to = 3.5, n = 501L, ylab = 'Density')
```

When $h>0$, domain is $(-\infty, \infty)$.

```{r}
curve(dGH(x, g = .3, h = .1), from = -2.5, to = 3.5, n = 501L, ylab = 'Density')
```

## Quantile

```{r}
curve(qGH(x, g = .3, h = .1), from = 0, to = 1, n = 501L, ylab = 'Quantile')
```

## Simulation

```{r}
set.seed(17); rGH(n = 1e3L, g = .3, h = .1) |> 
  hist(breaks = 30L, xlab = NULL, main = 'TukeyGH(g = .3, h = .1)')
```

# Letter-Value Based Estimates (@Hoaglin85)

Consider a random sample $T$ from Tukey's $g$-&-$h$ distribution with parameters $(A,B,g,h)$. The location parameter estimate $\hat{A}$ is simply the sample `median`, thus we subtract $\hat{A}$ from the original sample, and focus on the estimation of $(B, g, h)$.

Let $\hat{t}_p$ be the $p$-th sample quantile, for $0<p<.5$. Note that we have enforced that $\hat{t}_{.5}=0$.

We refer $t_p$ (or $-t_p$) as the lower half-spread (LHS) and $t_{1-p}$ as the upper half-spread (UHS).

## Tukey's $h$-Model

From Tukey's $h$-transformation (i.e., $g=0$) on LHS and UHS, as well as their difference,

$$
\begin{cases}
t_p = B~z_p~e^{hz^2_p/2} & \Rightarrow\quad 
\ln\left(\dfrac{\hat{t}_p}{z_p}\right) = {\color{blue}{\ln B_{\text{L}}}}+{\color{blue}{h_{\text{L}}}}\cdot z^2_p/2 + \varepsilon\\
t_{1-p} = B~z_{1-p}~e^{hz^2_{1-p}/2} & \Rightarrow\quad \ln\left(\dfrac{\hat{t}_{1-p}}{-z_p}\right) = {\color{blue}{\ln B_{\text{U}}}}+{\color{blue}{h_{\text{U}}}}\cdot z^2_p/2 + \varepsilon\\
t_{1-p}-t_p = B(z_{1-p}-z_p)e^{hz^2_p/2} & \Rightarrow\quad \ln\left(\dfrac{\hat{t}_{1-p} - \hat{t}_p}{-2z_p}\right) = {\color{blue}{\ln B}} + {\color{blue}{h}}\cdot z^2_p/2 + \varepsilon
\end{cases}
$$

We may perform any of these three least squares regressions and let $(\ln\hat{B}, \hat{h})$ be the estimated intercept and slope parameters (@Hoaglin85, equations (26a), (26b), (27) and (28)).

```{r}
#| fig-height: 5
#| fig-width: 4.5
set.seed(63); rGH(1e3L, h = .2) |>
  letterValue_h()
```

## Tukey's $g$-Model

From Tukey's $g$-transformation (i.e., $h=0$) on LHS and UHS, as well as their ratio,

$$
\begin{cases}
t_p = B(e^{gz_p}-1)/g\\
t_{1-p} = B(e^{gz_{1-p}}-1)/g\\
\dfrac{-t_p}{t_{1-p}} = \dfrac{1-e^{gz_p}}{e^{-gz_p}-1} = e^{gz_p} & \Rightarrow\quad g = \dfrac{1}{z_{p}}\cdot \ln\left(\dfrac{-t_{p}}{t_{1-p}}\right)
\end{cases}
$$

Let (@Hoaglin85, equation (10))

$$
\hat{g}_p = \dfrac{1}{z_{p}}\cdot \ln\left(\dfrac{-\hat{t}_{p}}{\hat{t}_{1-p}}\right)
$$

The skewness parameter estimate $\hat{g}$ is the `median` of $\hat{g}_p$.

Let $\hat{B}$ be the slope estimate of either or both of these two least squares regression with a fixed-zero intercept term (@Hoaglin85, equations (8a) and (8b)).

$$
\begin{cases}
\hat{t}_p = 0 + {\color{blue}{B_{\text{L}}}}\cdot (e^{\hat{g}z_p}-1)/\hat{g} + \varepsilon\\
\hat{t}_{1-p} = 0 + {\color{blue}{B_{\text{U}}}}\cdot (e^{\hat{g}z_{1-p}}-1)/\hat{g} + \varepsilon
\end{cases}
$$

```{r}
#| fig-height: 5
#| fig-width: 9
set.seed(43); rGH(1e3L, g = .3) |>
  letterValue_g()
```

## Tukey's $g$-&-$h$ Model

From Tukey's $g$-&-$h$ transformation on LHS and UHS, as well as their difference and ratio,

$$
\begin{cases}
t_p = Bg^{-1}(e^{gz_p}-1)e^{hz^2_p/2}\\
t_{1-p} = Bg^{-1}(e^{gz_{1-p}}-1)e^{hz^2_{1-p}/2}\\
t_{1-p}-t_p = Bg^{-1}(e^{-gz_p}-e^{gz_p})e^{hz^2_p/2}\\
\dfrac{-t_p}{t_{1-p}} = \dfrac{1-e^{gz_p}}{e^{-gz_p}-1} = e^{gz_p}
\end{cases}
$$

Given a skewness parameter estimate $\hat{g}$, let $(\ln\hat{B}, \hat{h})$ be the estimated intercept and slope parameters of either of these three least squares regression models (Upper: p487,eq(33). Lower: see p486,bottom)

$$
\begin{cases}
\ln\left(\dfrac{\hat{g}\hat{t}_p}{e^{\hat{g}z_p}-1}\right) = {\color{blue}{\ln B_{\text{L}}}}+{\color{blue}{h_{\text{L}}}}\cdot z^2_p/2 + \varepsilon\\
\ln\left(\dfrac{\hat{g}\hat{t}_{1-p}}{e^{-\hat{g}z_p}-1}\right) = {\color{blue}{\ln B_{\text{U}}}}+{\color{blue}{h_{\text{U}}}}\cdot z^2_p/2 + \varepsilon\\
\ln\left(\dfrac{\hat{g}\left(\hat{t}_{1-p}-\hat{t}_p\right)}{e^{-\hat{g}z_p}-e^{\hat{g}z_p}}\right) = {\color{blue}{\ln B}} + {\color{blue}{h}}\cdot z^2_p/2 + \varepsilon
\end{cases}
$$

These three least squares regression models could be very sensitive to the choice of $\hat{g}$. We explore some choices of $\hat{g}$ with parameter `g_select` of function `letterValue_gh()`,

-   `g_select = 'median'` of $\hat{g}_p$ (@Hoaglin85, equation (31)), the ***naive*** choice
-   `g_select = 'B_optim'`, to minimize $\text{err}_B =(\hat{B}_{\text{U}}-\hat{B}_{\text{L}})^2$
-   `g_select = 'h_optim'`, to minimize $\text{err}_h =(\hat{h}_{\text{U}}-\hat{h}_{\text{L}})^2$
-   `g_select = 'true'` simulating parameter $g$

In the following examples, the parameter `g_select = 'demo'` indicates an demonstration of estimates $(\hat{B}, \hat{h})$, $(\hat{B}_{\text{L}}, \hat{h}_{\text{L}})$ and $(\hat{B}_{\text{U}}, \hat{h}_{\text{U}})$ based on all available choices of $\hat{g}$.

### Good example

@fig-gh54 shows a 'good' data set that

-   $\hat{g}_p$ does not have a clear pattern (panel `E`);
-   we have very small $\text{err}_h$ and $\text{err}_B$ with the naive `'median'` choice of $\hat{g}$ (panel `A`);
-   difference choice of $\hat{g}$, e.g., `'B_optim'`, `'h_optim'` or `'true'` (panel `B`, `C`, `D`), does not affect much the estimates $\hat{B}$ and $\hat{h}$.

```{r}
#| label: fig-gh54
#| fig-height: 8
#| fig-width: 9
#| fig-cap: 'A good data set'
set.seed(54); rGH(n = 1e3L, g = -.3, h = .1) |> 
  letterValue_gh(g_select = 'demo', true = c(B = 1, g = -.3, h = .1))
```

### Salvageable example

@fig-gh335 shows a 'salvageable' data set that

-   $\hat{g}_p$ has a clear pattern (panel `E`);
-   estimated slope $\hat{h}<0$ for `'median'` and `'B_optim'` choices of $\hat{g}$ (panel `A`, `B`);
-   estimated slope $\hat{h}>0$ for `'h_optim'` choice of $\hat{g}$ (panel `C`), also rather close to the `'true'` choice of $\hat{g}$ (panel `D`).

```{r}
#| label: fig-gh335
#| fig-height: 8
#| fig-width: 9
#| fig-cap: 'A salvageable data set'
set.seed(335); rGH(n = 1e3L, g = -.3, h = .1) |>
  letterValue_gh(g_select = 'demo', true = c(B = 1, g = -.3, h = .1))
```

### Hopeless example

@fig-gh39 shows a 'hopeless' data set that

-   $\hat{g}_p$ has a clear pattern (panel `E`);
-   estimated slope $\hat{h}<0$ for all, including the `'true'`, choices of $\hat{g}$ (panel `A`, `B`, `C`, `D`).

```{r}
#| label: fig-gh39
#| fig-height: 8
#| fig-width: 9
#| fig-cap: 'A hopeless data set'
set.seed(39); rGH(n = 1e3L, g = -.3, h = .1) |>
  letterValue_gh(g_select = 'demo', true = c(B = 1, g = -.3, h = .1))
```

### Our recommendation

1.  Make `g_select = 'h_optim'` the default argument of function `letterValue_gh()`;
2.  If the estimated slope $\hat{h}<0$ for any user-choice of $\hat{g}$, then return $\hat{h}=0$
